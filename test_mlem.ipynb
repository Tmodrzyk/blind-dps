{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from guided_diffusion.blind_condition_methods import get_conditioning_method\n",
    "from guided_diffusion.measurements import get_operator, get_noise\n",
    "\n",
    "# Here replaces the regular unet by our trained unet\n",
    "# from guided_diffusion.unet import create_model\n",
    "import guided_diffusion.diffusion_model_unet \n",
    "import guided_diffusion.unet\n",
    "\n",
    "from guided_diffusion.gaussian_diffusion import create_sampler\n",
    "from data.dataloader import get_dataset, get_dataloader\n",
    "from motionblur.motionblur import Kernel\n",
    "from util.img_utils import Blurkernel, clear_color\n",
    "from util.logger import get_logger\n",
    "from skimage.restoration import richardson_lucy, wiener, unsupervised_wiener\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.signal import convolve\n",
    "\n",
    "device = 'cuda'\n",
    "    \n",
    "name = 'ellipse'\n",
    "root = '/home/modrzyk/code/data/ellipses/img/val/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = get_dataset(name=name, root=root, transforms=transform)\n",
    "loader = get_dataloader(dataset, batch_size=1, num_workers=0, train=False)\n",
    "\n",
    "# set seed for reproduce\n",
    "# set seed for reproduce\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.backends.cudnn.deterministic = True  # if using CUDA\n",
    "\n",
    "\n",
    "\n",
    "def wiener_deconv(x_0_hat, steps, **kwargs):\n",
    "    img = x_0_hat['img'].numpy().astype(np.float32)\n",
    "    kernel = x_0_hat['kernel'].numpy().astype(np.float32)\n",
    "    \n",
    "    deconv_img = wiener(image=img, balance=1.0, psf=kernel, clip=False)\n",
    "    # deconv_img, _ = unsupervised_wiener(image=img.numpy(), psf=kernel.numpy(), clip=False)\n",
    "    \n",
    "    x_0_hat['img'] = torch.from_numpy(deconv_img).to(device)\n",
    "    \n",
    "    return x_0_hat\n",
    "\n",
    "# def blind_mlem(x_0_hat, steps, clip, filter_epsilon, **kwargs):\n",
    "#     img = x_0_hat['img'].numpy()\n",
    "#     psf = x_0_hat['kernel'].numpy()\n",
    "    \n",
    "#     float_type = img.dtype\n",
    "#     image = img.astype(float_type, copy=False)\n",
    "#     # im_deconv = np.full(img.shape, 0.5, dtype=float_type)\n",
    "#     im_deconv = image.copy()\n",
    "#     im_mirror = np.flip(im_deconv)\n",
    "#     # psf_deconv = np.full(psf.shape, 0.5, dtype=float_type)\n",
    "#     psf_deconv = psf.copy()\n",
    "#     psf_deconv /= psf.sum()\n",
    "#     psf_mirror = np.flip(psf_deconv)\n",
    "\n",
    "#     # Small regularization parameter used to avoid 0 divisions\n",
    "#     eps = 1e-12\n",
    "\n",
    "#     for _ in range(steps):\n",
    "        \n",
    "#         # Update the kernel\n",
    "        \n",
    "#         conv = convolve(im_deconv, psf_deconv, mode='same') + eps\n",
    "        \n",
    "#         if filter_epsilon:\n",
    "#             relative_blur = np.where(conv < filter_epsilon, 0, image / conv)\n",
    "#         else: \n",
    "#             relative_blur = image / conv\n",
    "        \n",
    "#         conv2 = convolve(relative_blur, im_mirror, mode='same') + eps\n",
    "#         print(conv2.shape)\n",
    "#         psf_deconv *= conv2\n",
    "#         coeff = convolve(np.ones_like(psf_deconv), im_deconv, mode='same') + eps\n",
    "#         psf_deconv /= coeff\n",
    "#         psf_mirror = np.flip(psf_deconv)\n",
    "        \n",
    "#         # Update the image\n",
    "#         conv = convolve(im_deconv, psf_deconv, mode='same') + eps\n",
    "#         if filter_epsilon:\n",
    "#             relative_blur = np.where(conv < filter_epsilon, 0, image / conv)\n",
    "#         else:\n",
    "#             relative_blur = image / conv\n",
    "#         im_deconv *= convolve(relative_blur, psf_mirror, mode='same')\n",
    "#         coeff = convolve(np.ones_like(im_deconv), psf_mirror, mode='same') + eps\n",
    "#         im_deconv /= coeff\n",
    "#         im_mirror = np.flip(im_deconv)\n",
    "        \n",
    "#     if clip:\n",
    "#         im_deconv[im_deconv > 1] = 1\n",
    "#         im_deconv[im_deconv < -1] = -1\n",
    "    \n",
    "#     x_0_hat['img'] = torch.from_numpy(im_deconv).to(device)\n",
    "#     x_0_hat['kernel'] = torch.from_numpy(psf_deconv).to(device)\n",
    "    \n",
    "#     return x_0_hat\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "def richardson_lucy_blind(image, psf, original, num_iter=50):    \n",
    "    im_deconv = original.copy()    # init output\n",
    "    for i in range(num_iter):\n",
    "        psf_mirror = np.flip(psf)\n",
    "        conv = fftconvolve(im_deconv, psf, mode='same')\n",
    "        relative_blur = image / conv\n",
    "        im_deconv *= fftconvolve(relative_blur, psf_mirror, mode='same')\n",
    "        im_deconv_mirror = np.flip(im_deconv)\n",
    "        psf *= fftconvolve(relative_blur, im_deconv_mirror, mode='same')    \n",
    "    return im_deconv, psf\n",
    "\n",
    "def blind_mlem(x_0_hat, steps, clip, filter_epsilon, **kwargs):\n",
    "    img = x_0_hat['img'].numpy()\n",
    "    psf = x_0_hat['kernel'].numpy()\n",
    "    \n",
    "    im_deconv, psf_deconv = richardson_lucy_blind(img, psf, img, num_iter=steps)\n",
    "    x_0_hat['img'] = torch.from_numpy(im_deconv).to(device)\n",
    "    x_0_hat['kernel'] = torch.from_numpy(psf_deconv).to(device)\n",
    "    \n",
    "    return x_0_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def mlem(observation, x_0_hat, steps, clip, filter_epsilon, **kwargs):\n",
    "    img = x_0_hat['img']\n",
    "    kernel = x_0_hat['kernel']\n",
    "    \n",
    "    image = observation.cpu().numpy().astype(np.float32, copy=True)\n",
    "    psf = kernel.cpu().numpy().astype(np.float32, copy=False)\n",
    "    # im_deconv = np.full(image.shape, 0.5, dtype=np.float32)\n",
    "    im_deconv = img.cpu().numpy().astype(np.float32, copy=True)\n",
    "    psf_mirror = np.flip(psf)\n",
    "\n",
    "    # Small regularization parameter used to avoid 0 divisions\n",
    "    eps = 1e-6\n",
    "\n",
    "    for _ in range(steps):\n",
    "        conv = convolve(im_deconv, psf, mode='same', method='fft') + eps\n",
    "        if filter_epsilon:\n",
    "            relative_blur = np.where(conv < filter_epsilon, 0, image / conv)\n",
    "        else:\n",
    "            relative_blur = image / conv\n",
    "        im_deconv *= convolve(relative_blur, psf_mirror, mode='same', method='fft')\n",
    "\n",
    "    if clip:\n",
    "        im_deconv[im_deconv > 1] = 1\n",
    "        im_deconv[im_deconv < -1] = -1\n",
    "\n",
    "    x_0_hat['img'] = torch.from_numpy(im_deconv).to(device)\n",
    "    \n",
    "    return x_0_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def mlem_gpu(observation, x_0_hat, steps, clip, filter_epsilon, device):\n",
    "    img = x_0_hat['img']\n",
    "    # kernel = x_0_hat['kernel'].repeat(1,3,1,1)\n",
    "    kernel = x_0_hat['kernel']\n",
    "    \n",
    "\n",
    "    image = observation.to(torch.float32).clone().to(device)\n",
    "    psf = kernel.to(torch.float32).clone().to(device)\n",
    "    im_deconv = img.to(torch.float32).clone().to(device)\n",
    "    psf_mirror = torch.flip(psf, dims=[0, 1])\n",
    "\n",
    "    # Small regularization parameter used to avoid 0 divisions\n",
    "    eps = 1e-12\n",
    "    with torch.no_grad():\n",
    "        pad = (psf.size(2) // 2, psf.size(2) // 2, psf.size(3) // 2, psf.size(3) // 2)\n",
    "        for _ in range(steps):\n",
    "            conv = F.conv2d(F.pad(im_deconv, pad, mode='replicate'), psf) + eps\n",
    "            if filter_epsilon:\n",
    "                relative_blur = torch.where(conv < filter_epsilon, torch.tensor(0.0, device=device), image / conv)\n",
    "            else:\n",
    "                relative_blur = image / conv\n",
    "            im_deconv *= F.conv2d(F.pad(relative_blur, pad, mode='replicate'), psf_mirror)\n",
    "\n",
    "        if clip:\n",
    "            im_deconv = torch.clamp(im_deconv, -1, 1)\n",
    "\n",
    "        x_0_hat['img'] = im_deconv.to(device)\n",
    "    \n",
    "    return x_0_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "kernel_size = 61\n",
    "kernel_std = 3.0\n",
    "noise_level = 0.05\n",
    "\n",
    "operator = get_operator(name='gaussian_blur', kernel_size=kernel_size, intensity=kernel_std, device=device)\n",
    "noiser = get_noise(name='gaussian', sigma=noise_level)\n",
    "\n",
    "# Do Inference\n",
    "for i, ref_img in enumerate(loader):\n",
    "    \n",
    "    conv = Blurkernel('gaussian', kernel_size=kernel_size, std=kernel_std, device=device)\n",
    "    kernel = conv.get_kernel().type(torch.float32)\n",
    "    kernel = kernel.to(device).view(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "    ref_img = ref_img.to(device)\n",
    "    y_conv = operator.forward(ref_img)\n",
    "    y = noiser(y_conv)\n",
    "\n",
    "    y_mlem = y.clone()\n",
    "    y_mlem = torch.clamp(y_mlem, min=0)\n",
    "    \n",
    "    # Call mlem\n",
    "    x_0_hat={'img': y_mlem, 'kernel': kernel}\n",
    "    \n",
    "    x_0_hat = mlem_gpu(y_mlem, x_0_hat, steps=50, clip=False, filter_epsilon=1e-2, device=device)\n",
    "        \n",
    "    # Specify the output directory\n",
    "    output_dir = './results/ellipse/richardson-lucy/'\n",
    "    label_dir = os.path.join(output_dir, 'label')\n",
    "    input_dir = os.path.join(output_dir, 'input')\n",
    "    recon_dir = os.path.join(output_dir, 'recon')\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    os.makedirs(recon_dir, exist_ok=True)\n",
    "\n",
    "    # Save the images\n",
    "    output_path_original = os.path.join(output_dir, 'label', f'{str(i).zfill(5)}.png')\n",
    "    output_path_measurement = os.path.join(output_dir, 'input', f'{str(i).zfill(5)}.png')\n",
    "    output_path_mlem = os.path.join(output_dir, 'recon', f'{str(i).zfill(5)}.png')\n",
    "\n",
    "    # Save the original image\n",
    "    original_img = ref_img.squeeze().cpu().numpy()\n",
    "    plt.imsave(output_path_original, original_img, cmap='gray')\n",
    "\n",
    "    # Save the measurement image\n",
    "    measurement_img = y_mlem.squeeze().detach().cpu().numpy()\n",
    "    plt.imsave(output_path_measurement, measurement_img, cmap='gray')\n",
    "\n",
    "    # Save the MLEM image\n",
    "    mlem_img = x_0_hat['img'].squeeze().detach().cpu().numpy()\n",
    "    plt.imsave(output_path_mlem, mlem_img, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BlindDPS_simple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
